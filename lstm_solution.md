# LSTM-based Next-Word Generator Solution

## Setting Up Parameters

First, let's set up our parameters based on the given information:
- Vocabulary size (V) = 5
- Hidden state size (h) = 2  
- Token embedding size (e) = 2
- Context size (c) = 3

## 1. Token Embedding Layer

For our token embedding layer, we need a matrix E ∈ ℝ^(V×e), which maps each token ID to a 2-dimensional embedding vector.

Let's define a sample embedding matrix:

```
E = [
    [0.1, 0.2],  # Token ID 0
    [0.3, 0.4],  # Token ID 1
    [0.5, 0.6],  # Token ID 2
    [0.7, 0.8],  # Token ID 3
    [0.9, 1.0]   # Token ID 4
]
```

Let's say our context consists of token IDs [1, 2, 3]. After applying the embedding layer:
- x₁ = E[1] = [0.3, 0.4]
- x₂ = E[2] = [0.5, 0.6]
- x₃ = E[3] = [0.7, 0.8]

## 2. LSTM Layer

For the LSTM computations, we need to define the weights and biases:

Since each LSTM gate takes concatenated input [h_{t-1}, x_t] with dimensions [2, 2] (total 4), we need:
- Forget gate: W_f ∈ ℝ^(2×4), b_f ∈ ℝ^2
- Input gate: W_i ∈ ℝ^(2×4), b_i ∈ ℝ^2
- Output gate: W_o ∈ ℝ^(2×4), b_o ∈ ℝ^2
- Cell candidate: W_c ∈ ℝ^(2×4), b_c ∈ ℝ^2

Let's define sample values:

```
W_f = [
    [0.1, 0.2, 0.3, 0.4],
    [0.5, 0.6, 0.7, 0.8]
]
b_f = [0.1, 0.2]

W_i = [
    [0.2, 0.3, 0.4, 0.5],
    [0.6, 0.7, 0.8, 0.9]
]
b_i = [0.2, 0.3]

W_o = [
    [0.3, 0.4, 0.5, 0.6],
    [0.7, 0.8, 0.9, 1.0]
]
b_o = [0.3, 0.4]

W_c = [
    [0.4, 0.5, 0.6, 0.7],
    [0.8, 0.9, 1.0, 1.1]
]
b_c = [0.4, 0.5]
```

Now, let's compute the LSTM operations for each step:

### Initial values:
- h₀ = [0.0, 0.0]
- C₀ = [0.0, 0.0]

### For t = 1:
- Input: x₁ = [0.3, 0.4]
- Concatenated input: [h₀, x₁] = [0.0, 0.0, 0.3, 0.4]

Forget gate:
- f₁ = σ(W_f · [h₀, x₁] + b_f)
- f₁ = σ([0.1, 0.2, 0.3, 0.4] · [0.0, 0.0, 0.3, 0.4]^T + [0.1, 0.2])
- f₁ = σ([0.1×0.0 + 0.2×0.0 + 0.3×0.3 + 0.4×0.4 + 0.1, 0.5×0.0 + 0.6×0.0 + 0.7×0.3 + 0.8×0.4 + 0.2])
- f₁ = σ([0.09 + 0.16 + 0.1, 0.21 + 0.32 + 0.2])
- f₁ = σ([0.35, 0.73])
- f₁ = [0.587, 0.675]

Input gate:
- i₁ = σ(W_i · [h₀, x₁] + b_i)
- i₁ = σ([0.2×0.0 + 0.3×0.0 + 0.4×0.3 + 0.5×0.4 + 0.2, 0.6×0.0 + 0.7×0.0 + 0.8×0.3 + 0.9×0.4 + 0.3])
- i₁ = σ([0.12 + 0.20 + 0.2, 0.24 + 0.36 + 0.3])
- i₁ = σ([0.52, 0.90])
- i₁ = [0.627, 0.711]

Cell candidate:
- C̃₁ = tanh(W_c · [h₀, x₁] + b_c)
- C̃₁ = tanh([0.4×0.0 + 0.5×0.0 + 0.6×0.3 + 0.7×0.4 + 0.4, 0.8×0.0 + 0.9×0.0 + 1.0×0.3 + 1.1×0.4 + 0.5])
- C̃₁ = tanh([0.18 + 0.28 + 0.4, 0.30 + 0.44 + 0.5])
- C̃₁ = tanh([0.86, 1.24])
- C̃₁ = [0.696, 0.846]

Cell state:
- C₁ = f₁ ⊙ C₀ + i₁ ⊙ C̃₁
- C₁ = [0.587 × 0.0 + 0.627 × 0.696, 0.675 × 0.0 + 0.711 × 0.846]
- C₁ = [0.436, 0.602]

Output gate:
- o₁ = σ(W_o · [h₀, x₁] + b_o)
- o₁ = σ([0.3×0.0 + 0.4×0.0 + 0.5×0.3 + 0.6×0.4 + 0.3, 0.7×0.0 + 0.8×0.0 + 0.9×0.3 + 1.0×0.4 + 0.4])
- o₁ = σ([0.15 + 0.24 + 0.3, 0.27 + 0.40 + 0.4])
- o₁ = σ([0.69, 1.07])
- o₁ = [0.666, 0.745]

Hidden state:
- h₁ = o₁ ⊙ tanh(C₁)
- h₁ = [0.666 × tanh(0.436), 0.745 × tanh(0.602)]
- h₁ = [0.666 × 0.410, 0.745 × 0.539]
- h₁ = [0.273, 0.402]

### For t = 2:
- Input: x₂ = [0.5, 0.6]
- Concatenated input: [h₁, x₂] = [0.273, 0.402, 0.5, 0.6]

Forget gate:
- f₂ = σ(W_f · [h₁, x₂] + b_f)
- f₂ = σ([0.1×0.273 + 0.2×0.402 + 0.3×0.5 + 0.4×0.6 + 0.1, 0.5×0.273 + 0.6×0.402 + 0.7×0.5 + 0.8×0.6 + 0.2])
- f₂ = σ([0.027 + 0.080 + 0.150 + 0.240 + 0.1, 0.137 + 0.241 + 0.350 + 0.480 + 0.2])
- f₂ = σ([0.597, 1.408])
- f₂ = [0.645, 0.804]

Input gate:
- i₂ = σ(W_i · [h₁, x₂] + b_i)
- i₂ = σ([0.2×0.273 + 0.3×0.402 + 0.4×0.5 + 0.5×0.6 + 0.2, 0.6×0.273 + 0.7×0.402 + 0.8×0.5 + 0.9×0.6 + 0.3])
- i₂ = σ([0.055 + 0.121 + 0.200 + 0.300 + 0.2, 0.164 + 0.281 + 0.400 + 0.540 + 0.3])
- i₂ = σ([0.876, 1.685])
- i₂ = [0.706, 0.844]

Cell candidate:
- C̃₂ = tanh(W_c · [h₁, x₂] + b_c)
- C̃₂ = tanh([0.4×0.273 + 0.5×0.402 + 0.6×0.5 + 0.7×0.6 + 0.4, 0.8×0.273 + 0.9×0.402 + 1.0×0.5 + 1.1×0.6 + 0.5])
- C̃₂ = tanh([0.109 + 0.201 + 0.300 + 0.420 + 0.4, 0.218 + 0.362 + 0.500 + 0.660 + 0.5])
- C̃₂ = tanh([1.430, 2.240])
- C̃₂ = [0.891, 0.977]

Cell state:
- C₂ = f₂ ⊙ C₁ + i₂ ⊙ C̃₂
- C₂ = [0.645 × 0.436 + 0.706 × 0.891, 0.804 × 0.602 + 0.844 × 0.977]
- C₂ = [0.281 + 0.629, 0.484 + 0.825]
- C₂ = [0.910, 1.309]

Output gate:
- o₂ = σ(W_o · [h₁, x₂] + b_o)
- o₂ = σ([0.3×0.273 + 0.4×0.402 + 0.5×0.5 + 0.6×0.6 + 0.3, 0.7×0.273 + 0.8×0.402 + 0.9×0.5 + 1.0×0.6 + 0.4])
- o₂ = σ([0.082 + 0.161 + 0.250 + 0.360 + 0.3, 0.191 + 0.322 + 0.450 + 0.600 + 0.4])
- o₂ = σ([1.153, 1.963])
- o₂ = [0.760, 0.877]

Hidden state:
- h₂ = o₂ ⊙ tanh(C₂)
- h₂ = [0.760 × tanh(0.910), 0.877 × tanh(1.309)]
- h₂ = [0.760 × 0.724, 0.877 × 0.863]
- h₂ = [0.550, 0.757]

### For t = 3:
- Input: x₃ = [0.7, 0.8]
- Concatenated input: [h₂, x₃] = [0.550, 0.757, 0.7, 0.8]

Forget gate:
- f₃ = σ(W_f · [h₂, x₃] + b_f)
- f₃ = σ([0.1×0.550 + 0.2×0.757 + 0.3×0.7 + 0.4×0.8 + 0.1, 0.5×0.550 + 0.6×0.757 + 0.7×0.7 + 0.8×0.8 + 0.2])
- f₃ = σ([0.055 + 0.151 + 0.210 + 0.320 + 0.1, 0.275 + 0.454 + 0.490 + 0.640 + 0.2])
- f₃ = σ([0.836, 2.059])
- f₃ = [0.698, 0.887]

Input gate:
- i₃ = σ(W_i · [h₂, x₃] + b_i)
- i₃ = σ([0.2×0.550 + 0.3×0.757 + 0.4×0.7 + 0.5×0.8 + 0.2, 0.6×0.550 + 0.7×0.757 + 0.8×0.7 + 0.9×0.8 + 0.3])
- i₃ = σ([0.110 + 0.227 + 0.280 + 0.400 + 0.2, 0.330 + 0.530 + 0.560 + 0.720 + 0.3])
- i₃ = σ([1.217, 2.440])
- i₃ = [0.772, 0.920]

Cell candidate:
- C̃₃ = tanh(W_c · [h₂, x₃] + b_c)
- C̃₃ = tanh([0.4×0.550 + 0.5×0.757 + 0.6×0.7 + 0.7×0.8 + 0.4, 0.8×0.550 + 0.9×0.757 + 1.0×0.7 + 1.1×0.8 + 0.5])
- C̃₃ = tanh([0.220 + 0.379 + 0.420 + 0.560 + 0.4, 0.440 + 0.681 + 0.700 + 0.880 + 0.5])
- C̃₃ = tanh([1.979, 3.201])
- C̃₃ = [0.962, 0.997]

Cell state:
- C₃ = f₃ ⊙ C₂ + i₃ ⊙ C̃₃
- C₃ = [0.698 × 0.910 + 0.772 × 0.962, 0.887 × 1.309 + 0.920 × 0.997]
- C₃ = [0.635 + 0.743, 1.161 + 0.917]
- C₃ = [1.378, 2.078]

Output gate:
- o₃ = σ(W_o · [h₂, x₃] + b_o)
- o₃ = σ([0.3×0.550 + 0.4×0.757 + 0.5×0.7 + 0.6×0.8 + 0.3, 0.7×0.550 + 0.8×0.757 + 0.9×0.7 + 1.0×0.8 + 0.4])
- o₃ = σ([0.165 + 0.303 + 0.350 + 0.480 + 0.3, 0.385 + 0.606 + 0.630 + 0.800 + 0.4])
- o₃ = σ([1.598, 2.821])
- o₃ = [0.832, 0.944]

Hidden state:
- h₃ = o₃ ⊙ tanh(C₃)
- h₃ = [0.832 × tanh(1.378), 0.944 × tanh(2.078)]
- h₃ = [0.832 × 0.883, 0.944 × 0.970]
- h₃ = [0.735, 0.916]

## 3. Logits Layer

Now, we compute the output logits using the final hidden state h₃:

Let's define a sample weight matrix and bias for the output layer:
```
W_out = [
    [0.1, 0.2],
    [0.3, 0.4],
    [0.5, 0.6],
    [0.7, 0.8],
    [0.9, 1.0]
]
b_out = [0.1, 0.2, 0.3, 0.4, 0.5]
```

Computing the logits:
- y = W_out · h₃ + b_out
- y = [
    [0.1×0.735 + 0.2×0.916 + 0.1],
    [0.3×0.735 + 0.4×0.916 + 0.2],
    [0.5×0.735 + 0.6×0.916 + 0.3],
    [0.7×0.735 + 0.8×0.916 + 0.4],
    [0.9×0.735 + 1.0×0.916 + 0.5]
  ]
- y = [
    [0.074 + 0.183 + 0.1],
    [0.221 + 0.366 + 0.2],
    [0.368 + 0.550 + 0.3],
    [0.515 + 0.733 + 0.4],
    [0.662 + 0.916 + 0.5]
  ]
- y = [0.357, 0.787, 1.218, 1.648, 2.078]

## 4. Final Token Prediction

Applying softmax to y:
- softmax(y) = exp(y) / sum(exp(y))
- exp(y) = [exp(0.357), exp(0.787), exp(1.218), exp(1.648), exp(2.078)]
- exp(y) = [1.429, 2.197, 3.380, 5.197, 7.988]
- sum(exp(y)) = 20.191
- softmax(y) = [0.071, 0.109, 0.167, 0.257, 0.396]

The token with the highest probability is token ID 4 with a probability of 0.396.

Therefore, given the context [1, 2, 3], our LSTM-based next-word generator predicts token ID 4 as the next word.